The organization of tasks with the objective to execute them simultaneously is called a parallel pattern in computing. This organization of task is called parallelism. These include:
Task Parallelism: different tasks or function run simultaneously, each doing a different job.
Data parallelism: The same task is performed on different pieces of data at the same time.
Pipeline Parallelism: Tasks are divided into stages, with each stage processing data and passing it to the next.
Divide and Conquer: A problem split into smaller parts, solved separately and combined.

Parallel patterns are significant because they help improve speed, efficiency and scalability of computing systems enabling multiple tasks to run at the simultaneously.
Operating Systems run multiple applications at the same time. (e.g. internet browsing, MP3 player and file download) is an example of task parallelism.
Graphics Processing/ GPR computing renders images and videos using thousands of parallel threads is a form of data parallelism.
An assembly line where multiple robotic arms perform different tasks such as in a car assembly station is a pipeline parallelism.
Detection of objects in images by breaking them into smaller regions demonstrates divide and conquer. 

Heterogeneous computing combines the advantages of both CPU and GPU to execute parallel computing patters. CPU excels in sequential processing and complex decision- making, while GPU excel at massively parallel workloads. This results in optimization of performance, reduce execution time and efficiently solve problems. 
The increase in performance by having GPU process large-scale parallel workloads while CPU processes complex logic make parallel computing very popular. It also creates resource utilization efficiency by balancing workload between serial and parallel execution. GPU complete parallel tasks faster with lower power consumption than CPUs making it energy efficient. It is also suitable for cloud computing, AI, scientific research and real-time applications.

